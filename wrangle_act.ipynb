{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling WeRateDogs\n",
    "\n",
    "##### By Ruby Villacorta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Gathering data](#gather)\n",
    "- [Assessing data](#assess)\n",
    "   - [Quality](#quality)\n",
    "   - [Tidiness](#tidiness)\n",
    "- [Cleaning data](#clean)\n",
    "- [Storing, Analyzing, and Visualizing](#storing)\n",
    "   - [Insight one & visualization](#one)\n",
    "   - [Insight two](#two)\n",
    "   - [Insight three & visualization](#three)\n",
    "   - [Insight four & visualization](#four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to put in practice what I learned in data wrangling data section from Udacity Data Analysis Nanodegree program. The dataset that is wrangled is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gathering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Twitter archive file:** download this file manually by clicking the following link: twitter_archive_enhanced.csv\n",
    "\n",
    "- **The tweet image predictions**, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "- **Twitter API & JSON:** Each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data you find interesting. Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. \n",
    "    Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Twitter archive file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8eb50269a5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "#Import all packages needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import tweepy \n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV file \n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.sort_values('timestamp')\n",
    "twitter_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Tweet image prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL downloaded programatically \n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('image-predictions.tsv', mode ='wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "#Read TSV file\n",
    "image_prediction = pd.read_csv('image-predictions.tsv', sep='\\t' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/28384588/twitter-api-get-tweets-with-specific-id\n",
    "\n",
    "auth = tweepy.OAuthHandler('5Uur0mo4ol2kB8yhtZ1VxXS0u', 'h8E7fSpXWiMoBel7G1ZOAeu4Mgru0v0MtxH5ehYE1RKM89SiBH')\n",
    "auth.set_access_token('303562412-ct9aNnU0FQR0UKJVn1i1W3Y8omqSewiQWUcRaygB', 'D3qslrbdOU5fqTOp951kOIuZbkeTPBodnjNYoEGFR63Ft')\n",
    "api = tweepy.API(auth, \n",
    "                 parser = tweepy.parsers.JSONParser(), \n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Twitter API & JSON**\n",
    "\n",
    "https://stackoverflow.com/questions/47612822/how-to-create-pandas-dataframe-from-twitter-search-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Tweepy status object based on Tweet ID and store in list\n",
    "list_of_tweets = []\n",
    "# Tweets that can't be found are saved in the list below:\n",
    "cant_find_tweets_for_those_ids = []\n",
    "for tweet_id in twitter_archive['tweet_id']:   \n",
    "    try:\n",
    "        list_of_tweets.append(api.get_status(tweet_id))\n",
    "    except Exception as e:\n",
    "        cant_find_tweets_for_those_ids.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The list of tweets\" ,len(list_of_tweets))\n",
    "print(\"The list of tweets no found\" , len(cant_find_tweets_for_those_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then in this code block we isolate the json part of each tweepy \n",
    "#status object that we have downloaded and we add them all into a list\n",
    "\n",
    "my_list_of_dicts = []\n",
    "for each_json_tweet in list_of_tweets:\n",
    "    my_list_of_dicts.append(each_json_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we write this list into a txt file:\n",
    "\n",
    "with open('tweet_json.txt', 'w') as file:\n",
    "        file.write(json.dumps(my_list_of_dicts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify information of interest from JSON dictionaries in txt file\n",
    "#and put it in a dataframe called tweet JSON\n",
    "my_demo_list = []\n",
    "with open('tweet_json.txt', encoding='utf-8') as json_file:  \n",
    "    all_data = json.load(json_file)\n",
    "    for each_dictionary in all_data:\n",
    "        tweet_id = each_dictionary['id']\n",
    "        whole_tweet = each_dictionary['text']\n",
    "        only_url = whole_tweet[whole_tweet.find('https'):]\n",
    "        favorite_count = each_dictionary['favorite_count']\n",
    "        retweet_count = each_dictionary['retweet_count']\n",
    "        followers_count = each_dictionary['user']['followers_count']\n",
    "        friends_count = each_dictionary['user']['friends_count']\n",
    "        whole_source = each_dictionary['source']\n",
    "        only_device = whole_source[whole_source.find('rel=\"nofollow\">') + 15:-4]\n",
    "        source = only_device\n",
    "        retweeted_status = each_dictionary['retweeted_status'] = each_dictionary.get('retweeted_status', 'Original tweet')\n",
    "        if retweeted_status == 'Original tweet':\n",
    "            url = only_url\n",
    "        else:\n",
    "            retweeted_status = 'This is a retweet'\n",
    "            url = 'This is a retweet'\n",
    "\n",
    "        my_demo_list.append({'tweet_id': str(tweet_id),\n",
    "                             'favorite_count': int(favorite_count),\n",
    "                             'retweet_count': int(retweet_count),\n",
    "                             'followers_count': int(followers_count),\n",
    "                             'friends_count': int(friends_count),\n",
    "                             'url': url,\n",
    "                             'source': source,\n",
    "                             'retweeted_status': retweeted_status,\n",
    "                            })\n",
    "        tweet_json = pd.DataFrame(my_demo_list, columns = ['tweet_id', 'favorite_count','retweet_count', \n",
    "                                                           'followers_count', 'friends_count','source', \n",
    "                                                           'retweeted_status', 'url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual assessment\n",
    "\n",
    "Each piece of gathered data is displayed in the Jupyter Notebook for visual assessment purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic assessment\n",
    "\n",
    "Pandas' functions and/or methods are used to assess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(twitter_archive['tweet_id'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_archive.loc[twitter_archive.rating_numerator == 204, 'text']) \n",
    "print(twitter_archive.loc[twitter_archive.rating_numerator == 143, 'text']) \n",
    "print(twitter_archive.loc[twitter_archive.rating_numerator == 666, 'text']) \n",
    "print(twitter_archive.loc[twitter_archive.rating_numerator == 1176, 'text'])\n",
    "print(twitter_archive.loc[twitter_archive.rating_numerator == 144, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print whole text in order to verify numerators and denominators\n",
    "print(twitter_archive['text'][1120]) #17 dogs\n",
    "print(twitter_archive['text'][1634]) #13 dogs\n",
    "print(twitter_archive['text'][313]) #just a tweet to explain actual ratings, this will be ignored when cleaning data\n",
    "print(twitter_archive['text'][189]) #no picture, this will be ignored when cleaning data\n",
    "print(twitter_archive['text'][1779]) #12 dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_archive.loc[twitter_archive.rating_denominator == 11, 'text']) \n",
    "print(twitter_archive.loc[twitter_archive.rating_denominator == 2, 'text']) \n",
    "print(twitter_archive.loc[twitter_archive.rating_denominator == 16, 'text']) \n",
    "print(twitter_archive.loc[twitter_archive.rating_denominator == 15, 'text'])\n",
    "print(twitter_archive.loc[twitter_archive.rating_denominator == 7, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_archive['text'][784]) #retweet - it will be deleted when delete all retweets\n",
    "print(twitter_archive['text'][1068]) #actual rating 14/10 need to change manually\n",
    "print(twitter_archive['text'][1662]) #actual rating 10/10 need to change manually\n",
    "print(twitter_archive['text'][2335]) #actual rating 9/10 need to change manually\n",
    "print(twitter_archive['text'][1663]) # tweet to explain rating\n",
    "print(twitter_archive['text'][342]) #no rating - delete\n",
    "print(twitter_archive['text'][516]) #no rating - delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   UDACITY REVIEWER POINT: \n",
    "#Check the numerator - in the source data there seems to be fractional numerators. Are those preserved \n",
    "#when exporting the data? Take a look at the id = 681340665377193000\n",
    "\n",
    "#TWITTER_ID NOT FOUND\n",
    "\n",
    "twitter_archive[twitter_archive['tweet_id'] == 681340665377193000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewer point brought something to my attention that it was not picked up during the first assessment\n",
    "\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(twitter_archive[twitter_archive['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]\n",
    "            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(image_prediction.jpg_url.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(g for _, g in image_prediction.groupby(\"jpg_url\") if len(g) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_prediction.p1_dog.value_counts())\n",
    "print(image_prediction.p2_dog.value_counts())\n",
    "print(image_prediction.p3_dog.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction.img_num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.retweeted_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quality'></a>\n",
    "### Quality\n",
    "\n",
    "*Completeness, validity, accuracy, consistency (content issues)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *twitter_archive*\n",
    "1. Keep original ratings (no retweets) that have images\n",
    "- Delete columns that won't be used for analysis\n",
    "- Erroneous datatypes (doggo, floofer, pupper and puppo columns)\n",
    "- Separate timestamp into day - month - year (3 columns)\n",
    "- Correct numerators with decimals\n",
    "- Correc denominators other than 10: \n",
    "    \n",
    "    a. Manually (few examples assessed by individual print text).\n",
    "    \n",
    "    b. Programatically (Tweets with denominator not equal to 10 are usually multiple dogs). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *image_prediction*\n",
    "6. Drop 66 jpg_url duplicated\n",
    "7. Create 1 column for image prediction and 1 column for confidence level\n",
    "8. Delete columns that won't be used for analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *tweet_json*\n",
    "1. Keep original tweets only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tidiness'></a>\n",
    "### Tidiness \n",
    "\n",
    "1. Change tweet_id to type int64 in order to merge with the other 2 tables\n",
    "- All tables should be part of one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = twitter_archive.copy()\n",
    "image_prediction_clean = image_prediction.copy()\n",
    "tweet_json_clean = tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Twitter archive** - keep original ratings (no retweets) that have images. \n",
    "\n",
    "Based on info, there are 181 values in retweeted_status_id and retweeted_status_user_id. Delete the retweets. Once I merge twitter_archive and image_prediction, I will only keep the ones with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Delete retweets by filtering the NaN of retweeted_status_user_id\n",
    "twitter_archive_clean = twitter_archive_clean[pd.isnull(twitter_archive_clean['retweeted_status_user_id'])]\n",
    "\n",
    "#TEST\n",
    "print(sum(twitter_archive_clean.retweeted_status_user_id.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Twitter archive** - Delete columns that won't be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the column names of twitter_archive_clean\n",
    "print(list(twitter_archive_clean))\n",
    "\n",
    "#CODE: Delete columns no needed\n",
    "twitter_archive_clean = twitter_archive_clean.drop(['source',\n",
    "                                                    'in_reply_to_status_id',\n",
    "                                                    'in_reply_to_user_id',\n",
    "                                                    'retweeted_status_id',\n",
    "                                                    'retweeted_status_user_id', \n",
    "                                                    'retweeted_status_timestamp', \n",
    "                                                    'expanded_urls'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "list(twitter_archive_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Twitter_archive** - Erroneous datatypes (doggo, floofer, pupper and puppo columns)\n",
    "\n",
    "Melt the doggo, floofer, pupper and puppo columns to *dogs* and *dogs_stage* column. Then drop *dogs*. Sort by *dogs_stage* in order to then drop duplicated based on tweet_id except for the last occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Melt the doggo, floofer, pupper and puppo columns to dogs and dogs_stage column\n",
    "twitter_archive_clean = pd.melt(twitter_archive_clean, id_vars=['tweet_id',                                          \n",
    "                                                                'timestamp',\n",
    "                                                                'text',\n",
    "                                                                'rating_numerator',\n",
    "                                                                'rating_denominator',\n",
    "                                                                'name'],\n",
    "                               var_name='dogs', value_name='dogs_stage')\n",
    "\n",
    "#CODE: drop dogs\n",
    "twitter_archive_clean = twitter_archive_clean.drop('dogs', 1)\n",
    "\n",
    "#CODE: Sort by dogs_stage then drop duplicated based on tweet_id except the last occurrence\n",
    "twitter_archive_clean = twitter_archive_clean.sort_values('dogs_stage').drop_duplicates(subset='tweet_id', \n",
    "                                                                                        keep='last')\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "twitter_archive_clean['dogs_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Twitter_archive** - Separate timestamp into day - month - year (3 columns)\n",
    "\n",
    "First convert *timestamp* to datetime. Then extract year, month and day to new columns. Finally drop *timestamp* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: convert timestamp to datetime\n",
    "twitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp'])\n",
    "\n",
    "#extract year, month and day to new columns\n",
    "twitter_archive_clean['year'] = twitter_archive_clean['timestamp'].dt.year\n",
    "twitter_archive_clean['month'] = twitter_archive_clean['timestamp'].dt.month\n",
    "twitter_archive_clean['day'] = twitter_archive_clean['timestamp'].dt.day\n",
    "\n",
    "#Finally drop timestamp column\n",
    "twitter_archive_clean = twitter_archive_clean.drop('timestamp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "list(twitter_archive_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Twitter_archive** - Correc numerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "twitter_archive_clean[['rating_numerator', 'rating_denominator']] = twitter_archive_clean[['rating_numerator','rating_denominator']].astype(float)\n",
    "\n",
    "twitter_archive_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "\n",
    "#First change numerator and denominators type int to float to allow decimals \n",
    "twitter_archive_clean[['rating_numerator', 'rating_denominator']] = twitter_archive_clean[['rating_numerator','rating_denominator']].astype(float)\n",
    "\n",
    "#Update numerators\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 883482846933004288), 'rating_numerator'] = 13.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 786709082849828864), 'rating_numerator'] = 9.75\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 778027034220126208), 'rating_numerator'] = 11.27\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 681340665377193984), 'rating_numerator'] = 9.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 680494726643068929), 'rating_numerator'] = 11.26\n",
    "\n",
    "#TEST\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(twitter_archive_clean[twitter_archive_clean['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]\n",
    "            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Twitter_archive** - Correc denominators\n",
    "\n",
    "#### *a. Manually*\n",
    "Five tweets with denominator not equal to 10 for special circunstances. Update both numerators and denominators when necessary. Delete other five tweets because they do not have actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Update both numerators and denominators\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 740373189193256964), 'rating_numerator'] = 14\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 740373189193256964), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 682962037429899265), 'rating_numerator'] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 682962037429899265), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 666287406224695296), 'rating_numerator'] = 9\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 666287406224695296), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 722974582966214656), 'rating_numerator'] = 13\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 722974582966214656), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 716439118184652801), 'rating_numerator'] = 13.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 716439118184652801), 'rating_denominator'] = 10\n",
    "\n",
    "#CODE: Delete five tweets with no actual ratings\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 832088576586297345]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 810984652412424192]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 682808988178739200]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 835246439529840640]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 686035780142297088]\n",
    "\n",
    "#TEST: Left only the group dogs for programatically clean\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(twitter_archive_clean[twitter_archive_clean['rating_denominator'] != 10][['tweet_id',\n",
    "                                                                                      'text',\n",
    "                                                                                      'rating_numerator',\n",
    "                                                                                      'rating_denominator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *b. Programatically* \n",
    "These tweets with denominator not equal to 10 are multiple dogs. For example, tweet_id 713900603437621000 has numerator and denominators 99/90 because there are 9 dogs in the picture https://t.co/mpvaVxKmc1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Create a new column with rating in float type to avoid converting all int column to float\n",
    "twitter_archive_clean['rating'] = 10 * twitter_archive_clean['rating_numerator'] / twitter_archive_clean['rating_denominator'].astype(float)\n",
    "\n",
    "#TEST\n",
    "twitter_archive_clean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Image_prediction** - Drop 66 jpg_url duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Delete duplicated jpg_url\n",
    "image_prediction_clean = image_prediction_clean.drop_duplicates(subset=['jpg_url'], keep='last')\n",
    "\n",
    "#TEST\n",
    "sum(image_prediction_clean['jpg_url'].duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 8. Image_prediction** - Create 1 column for image prediction and 1 column for confidence level\n",
    "\n",
    "Create a function where I keep the first true prediction along the confidence level as new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: the first true prediction (p1, p2 or p3) will be store in these lists\n",
    "dog_type = []\n",
    "confidence_list = []\n",
    "\n",
    "#create a function with nested if to capture the dog type and confidence level\n",
    "# from the first 'true' prediction\n",
    "def image(image_prediction_clean):\n",
    "    if image_prediction_clean['p1_dog'] == True:\n",
    "        dog_type.append(image_prediction_clean['p1'])\n",
    "        confidence_list.append(image_prediction_clean['p1_conf'])\n",
    "    elif image_prediction_clean['p2_dog'] == True:\n",
    "        dog_type.append(image_prediction_clean['p2'])\n",
    "        confidence_list.append(image_prediction_clean['p2_conf'])\n",
    "    elif image_prediction_clean['p3_dog'] == True:\n",
    "        dog_type.append(image_prediction_clean['p3'])\n",
    "        confidence_list.append(image_prediction_clean['p3_conf'])\n",
    "    else:\n",
    "        dog_type.append('Error')\n",
    "        confidence_list.append('Error')\n",
    "\n",
    "#series objects having index the image_prediction_clean column.        \n",
    "image_prediction_clean.apply(image, axis=1)\n",
    "\n",
    "#create new columns\n",
    "image_prediction_clean['dog_type'] = dog_type\n",
    "image_prediction_clean['confidence_list'] = confidence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that has prediction_list 'error'\n",
    "image_prediction_clean = image_prediction_clean[image_prediction_clean['dog_type'] != 'Error']\n",
    "\n",
    "#TEST: \n",
    "image_prediction_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Image_prediction** - Delete columns that won't be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: print list of image_prediction columns\n",
    "print(list(image_prediction_clean))\n",
    "\n",
    "#Delete columns\n",
    "image_prediction_clean = image_prediction_clean.drop(['img_num', 'p1', \n",
    "                                                      'p1_conf', 'p1_dog', \n",
    "                                                      'p2', 'p2_conf', \n",
    "                                                      'p2_dog', 'p3', \n",
    "                                                      'p3_conf', \n",
    "                                                      'p3_dog'], 1)\n",
    "\n",
    "#TEST\n",
    "list(image_prediction_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Tweet_json** - keep 2174 original tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE:\n",
    "tweet_json_clean = tweet_json_clean[tweet_json_clean['retweeted_status'] == 'Original tweet']\n",
    "\n",
    "#TEST\n",
    "tweet_json_clean['retweeted_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Tidiness** - Change tweet_id to type int64 in order to merge with the other 2 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: change tweet_id from str to int\n",
    "tweet_json_clean['tweet_id'] = tweet_json_clean['tweet_id'].astype(int)\n",
    "\n",
    "#TEST\n",
    "tweet_json_clean['tweet_id'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Tidiness** - All tables should be part of one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: create a new dataframe that merge twitter_archive_clean and \n",
    "#image_prediction_clean\n",
    "df_twitter1 = pd.merge(twitter_archive_clean, \n",
    "                      image_prediction_clean, \n",
    "                      how = 'left', on = ['tweet_id'])\n",
    "\n",
    "#keep rows that have picture (jpg_url)\n",
    "df_twitter1 = df_twitter1[df_twitter1['jpg_url'].notnull()]\n",
    "\n",
    "#TEST\n",
    "df_twitter1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: create a new dataframe that merge df_twitter and tweet_json_clean\n",
    "df_twitter = pd.merge(df_twitter1, tweet_json_clean, \n",
    "                      how = 'left', on = ['tweet_id'])\n",
    "\n",
    "#TEST\n",
    "df_twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='storing'></a>\n",
    "## Storing, Analyzing, and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the clean DataFrame in a CSV file\n",
    "df_twitter.to_csv('twitter_archive_master.csv', \n",
    "                 index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='one'></a>\n",
    "### Insight one & visualization\n",
    "\n",
    "Golden retriever is the most common dog in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['dog_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_type = df_twitter.groupby('dog_type').filter(lambda x: len(x) >= 25)\n",
    "\n",
    "df_dog_type['dog_type'].value_counts().plot(kind = 'barh')\n",
    "plt.title('Histogram of the Most Rated Dog Type')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Type of dog')\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.savefig('output.png',bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='two'></a>\n",
    "### Insight two\n",
    "Japanese_spaniel has the lowest average rating\n",
    "Clumber has the highest average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_type_mean = df_twitter.groupby('dog_type').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_type_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_type_sorted = df_dog_type_mean['rating'].sort_values()\n",
    "\n",
    "df_dog_type_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_twitter.loc[df_twitter.dog_type == 'Japanese_spaniel', 'url']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter[df_twitter['dog_type'] == 'golden_retriever']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='three'></a>\n",
    "### Insight three & visualization\n",
    "\n",
    "Dog_types with low number of ratings show a high variaty of mean ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_type_count = df_twitter.groupby('dog_type').count()\n",
    "df_dog_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_type_count = df_dog_type_count['rating']\n",
    "dog_type_mean = df_dog_type_mean['rating']\n",
    "dog_type_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['dog_type_count'] = dog_type_count\n",
    "df['dog_type_mean'] = dog_type_mean\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='dog_type_count', y='dog_type_mean', kind='scatter')\n",
    "plt.xlabel('Number of Ratings of a Dog Type')\n",
    "plt.ylabel('Average Rating of Dog Type')\n",
    "plt.title('Average Rating of Dog Type by Number of Ratings of a Dog Type Scatter Plot')\n",
    "\n",
    "fig = plt.gcf()\n",
    "#plt.savefig('X:/' + newName + '.png', \n",
    "fig.savefig('output2.png',bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='four'></a>\n",
    "### Insight four & visualization\n",
    "\n",
    "The highest ratings do not receive the most retweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_twitter.plot(x='retweet_count', y='rating', kind='scatter')\n",
    "plt.xlabel('Retweet Counts')\n",
    "plt.ylabel('Ratings')\n",
    "plt.title('Retweet Counts by Ratings Scatter Plot')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('output3.png',bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
